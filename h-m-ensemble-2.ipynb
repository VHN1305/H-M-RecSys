{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"},{"sourceId":3235828,"sourceType":"datasetVersion","datasetId":1961559},{"sourceId":7292517,"sourceType":"datasetVersion","datasetId":4229596},{"sourceId":7305919,"sourceType":"datasetVersion","datasetId":4238941},{"sourceId":7351133,"sourceType":"datasetVersion","datasetId":4269066},{"sourceId":7351925,"sourceType":"datasetVersion","datasetId":4269608},{"sourceId":7352357,"sourceType":"datasetVersion","datasetId":4269895},{"sourceId":7361302,"sourceType":"datasetVersion","datasetId":4275938}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"contents\"></a>\n# Contents\n1. [Import libraries](#libs)\n2. [Load data](#df)\n\n3. [Preprocess](#preprocess)\n\n    3.1. [Embed texts into vectors using BERT](#etb)\n\n4. [Retrieval](#retrieval)\n    \n    4.1. [BERT + Neareast Neighbors](#bert_NN)\n    \n    4.2. [Recommend Items Repurchased](#rec_repur)\n    \n    4.3. [Trending Products Weekly](#trend)\n    \n    4.4. [SVD](#SVD)\n    \n5. [Rerank](#rerank)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"libs\"></a>\n# 1. Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom typing import List, Union, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom transformers import AutoTokenizer, AutoModel\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport glob\nimport reco\nfrom tqdm import tqdm\nimport datetime\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:02:07.451920Z","iopub.execute_input":"2024-01-10T07:02:07.452295Z","iopub.status.idle":"2024-01-10T07:02:13.113401Z","shell.execute_reply.started":"2024-01-10T07:02:07.452262Z","shell.execute_reply":"2024-01-10T07:02:13.112603Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id = \"df\"></a>\n# 2. Load data","metadata":{}},{"cell_type":"code","source":"# Defining the base paths.\nBASE_IN_PATH = \"/kaggle/input/h-and-m-personalized-fashion-recommendations\"\nBASE_OUT_PATH = \"/kaggle/working\"","metadata":{"execution":{"iopub.status.busy":"2024-01-08T18:57:20.314200Z","iopub.execute_input":"2024-01-08T18:57:20.315006Z","iopub.status.idle":"2024-01-08T18:57:20.319482Z","shell.execute_reply.started":"2024-01-08T18:57:20.314971Z","shell.execute_reply":"2024-01-08T18:57:20.318373Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"articles_df = pd.read_csv(os.path.join(BASE_IN_PATH, \"articles.csv\"))\ncustomers_df = pd.read_csv(os.path.join(BASE_IN_PATH, \"customers.csv\"))\ntransactions_df = pd.read_csv(os.path.join(BASE_IN_PATH, \"transactions_train.csv\"), dtype={\"article_id\": str, \"customer_id\": str}, parse_dates=[\"t_dat\"])\nsubmission_df = pd.read_csv(os.path.join(BASE_IN_PATH, \"sample_submission.csv\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T18:57:20.320759Z","iopub.execute_input":"2024-01-08T18:57:20.321085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"preprocess\"></a>\n# 3. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<a id = \"etb\"></a>\n## 3.1. Embed texts into vectors using BERT","metadata":{}},{"cell_type":"markdown","source":"The preprocessing pipeline will include:\n\n- Fill in the NaN values of `detail_desc` with an empty string \n- Merge *the chosen* textual columns into one called `text`\n- Lowercase the new column `text`\n- Using *BERT*, embed the values in `text` into vectors\n- Left join the transactions dataframe with the articles dataframe (aquire the new vector column)\n- Calculate the average transaction vector for each customer and add it to `customers_df` (or create a new DataFrame)\n- Add a new column called `bought_articles` in `customers_df`, in which all article IDs of the bough articles for each customer will be saved","metadata":{}},{"cell_type":"code","source":"# Let's start by filling the NaN values in `detail_desc` with an empty string.\narticles_df[\"detail_desc\"] = articles_df[\"detail_desc\"].fillna(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These were the selected textual columns to be merged.\ntext_cols = [\n    \"prod_name\",\n    \"product_type_name\",\n    \"product_group_name\",\n    \"graphical_appearance_name\",\n    \"colour_group_name\",\n    \"department_name\",\n    \"index_name\",\n    \"detail_desc\",\n]\n\ndef merge_text_columns(row, columns):\n    texts = []\n    \n    # Looping through the columns except for `detail_desc`.\n    # It will be appended with a '-' separator.\n    for col in columns[:-1]:\n        texts.append(row[col])\n        \n    texts = \", \".join(texts)\n    texts = \" - \".join([texts, row[columns[-1]]])\n    \n    return texts\n\narticles_df[\"text\"] = articles_df.apply(lambda row: merge_text_columns(row, text_cols), axis=1)\narticles_df[\"text\"].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowercase the newly created `text` column.\narticles_df[\"text\"] = articles_df[\"text\"].apply(lambda text: text.lower())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"retrieval\"></a>\n# 4. Retrieval\n","metadata":{}},{"cell_type":"markdown","source":"<a id = \"bert_NN\"></a>\n## 4.1. BERT + Neareast Neighbors","metadata":{}},{"cell_type":"code","source":"# The fraction of the articles that we are going to embed. I use a subset of the whole dataset\n# because I want to speed up the whole process. A larger subset might also be used, but the preprocessing\n# will take a lot more time.\nEMBED_FRAC = 0.1\n# If this is set to True, the `EMBED_FRAC` fraction of the dataset will be shuffled randomly.\nRANDOMNESS = False\n\n# Maximum length of a tokenized sequence. I chose these values based on the histograms above.\n# BERT uses a subword tokenizer, but still, a lot of samples have much less than 60 words.\nMAX_LEN = 60","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertVectorizer:\n    \n    def __init__(self):\n        self._model_id = \"bert-base-uncased\"\n        self._tokenizer = AutoTokenizer.from_pretrained(self._model_id)\n        self._base_model = AutoModel.from_pretrained(self._model_id)\n        \n    def embed(self, texts: List[str], max_length=60) -> np.ndarray:\n        \"\"\"Embed `text` into a vector of size 768.\n        Args:\n            text (List[str]): Input text.\n            max_length (int): The maximum length of a text in `texts`. Defaults to 60.\n        \n        Returns:\n            numpy.ndarray: The vector representation of `text`.\n        \"\"\"\n        # Since the input size vary, I pad or truncate, based on the lengths.\n        inputs = self._tokenizer(\n            texts, \n            max_length=max_length, \n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        # Getting the output tensor of the model. It is of shape (batch_size, seq_len, embedding_size).\n        # Then I get only the vectors for each [CLS] corresponding to each input in `text`.\n        embedding = self._base_model(**inputs).last_hidden_state[:, 0, :].detach()\n        # `output` shape: (batch_size, embedding_size)\n        \n        return embedding.numpy()\n\n\nbert = BertVectorizer()\nembedding = bert.embed(articles_df[\"text\"][:10].tolist(), max_length=MAX_LEN)\nprint(f\"Vector shape: {embedding.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I could use multithreading here. This method is too slow.\ndef create_embeddings(dataframe: pd.DataFrame, vectorizer: nn.Module, batch_size=5) -> pd.DataFrame:\n    vectors = []\n    \n    for i in tqdm(range(0, len(dataframe), batch_size)):\n        curr_df = dataframe.iloc[i:i + batch_size]\n        vectors.extend(vectorizer.embed(curr_df[\"text\"].tolist()))\n\n    dataframe[\"embedding\"] = vectors\n        \n    return dataframe\n\n\nif RANDOMNESS:\n    print(\"Shuffling the articles dataframe...\")\n    articles_sample = articles_df.sample(frac=EMBED_FRAC, random_state=1)\nelse:\n    articles_sample = articles_df.iloc[:int(EMBED_FRAC * len(articles_df))]\n        \nembedded_articles = create_embeddings(articles_sample, vectorizer=bert, batch_size=100)\nembedded_articles[\"embedding\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add the vectors into the transactions","metadata":{}},{"cell_type":"code","source":"# Getting the subset of `transactions_train.csv` which has these particular article IDs.\nsample_transaction_df = transactions_df[\n    transactions_df[\"article_id\"].isin(\n        embedded_articles[\"article_id\"].tolist()\n    )\n]\nprint(\"Num. of transactions with these article IDs:\", len(sample_transaction_df))\nsample_transaction_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Executing a left join on the transactions with the embedded articles.\n# This maps the embedding vectors to each transaction.\nembedded_transactions = sample_transaction_df.merge(\n    embedded_articles, \n    how=\"left\", left_on=\"article_id\", right_on=\"article_id\"\n)[[\n    \"customer_id\",\n    \"article_id\",\n    \"price\",\n    \"embedding\"\n]]\nembedded_transactions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate the average transaction vector for each customer","metadata":{}},{"cell_type":"code","source":"embedded_customer_ids = embedded_transactions[\"customer_id\"].unique().tolist()\nprint(f\"Num. Customers in the embedded transactions: {len(embedded_customer_ids)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_embeddings = embedded_transactions.groupby([\"customer_id\"])[\"embedding\"].apply(\n    lambda emb: emb.mean()\n).reset_index()\ncustomer_embeddings.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add new column for bought articles","metadata":{}},{"cell_type":"code","source":"# Group the article IDs based on the customer ID.\nembedded_transactions[\"article_id\"] = embedded_transactions[\"article_id\"].astype(str)\nbought_articles = embedded_transactions.groupby([\"customer_id\"]).agg({\n    \"article_id\": \",\".join\n})\nbought_articles.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the list of bought articles into the `customer_embeddings` DataFrame.\n# Here, it doesn't matter if it is inner, left or right, since the customer IDs are the same.\ncustomer_embeddings= customer_embeddings.merge(bought_articles, how=\"left\", left_on=\"customer_id\", right_on=\"customer_id\")\ncustomer_embeddings.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass SystemMetadata:\n    articles_metadata: pd.DataFrame\n    customers_metadata: pd.DataFrame\n\n\nclass ArticleRecommender:\n    \"\"\"Recommendation system for H&M products. Based on previous purchases it\n    suggests new products that the customers might like.\n    \n    Args:\n        metadata (SystemMetadata): Dataclass consisting of vectors describing each article and each customer.   \n    \"\"\"\n    \n    def __init__(self, metadata: SystemMetadata):\n        self._customers_metadata = metadata.customers_metadata\n        self._articles_metadata = metadata.articles_metadata\n        self._articles_metadata[\"article_id\"] = self._articles_metadata[\"article_id\"].astype(str)\n        \n        self._model = NearestNeighbors(n_neighbors=12)\n    \n    def recommend(self, customer_id: str, topk: int = 12) -> List[str]:\n        \"\"\"Recommends `topk` articles based on `customer_id`'s previous purchases.\n        \n        Args:\n            customer_id (str): ID of the customer to which you want to recommend new products.\n            topk (int): Denotes how many suggestions to make. They are ordered (top K) suggestions. Defaults to 12.\n        \n        Returns:\n            List[str]: List of article IDs.\n        \"\"\"\n        \n        # Creating deep copies, since I don't want to alter the original DataFrames.\n        # Also, when we call `recommend()` multiple times, each time we want to have\n        # all the metadata.\n        articles_metadata = self._articles_metadata.copy(deep=True)\n        customers_metadata = self._customers_metadata.copy(deep=True)\n        \n        # Getting the already purchased articles. We want to suggest new things to our Customers, right?\n        customer_purchases = self._get_customer_field_value(\n            customer_id=customer_id,\n            field_name=\"article_id\"\n        ).split(\",\")\n        \n        # Get the DataFrame IDs of the articles that were already purchased by this customer.\n        # Then, remove these entries from the DataFrames.\n        article_df_ids = self._articles_metadata[\n            self._articles_metadata[\"article_id\"].isin(customer_purchases)\n        ].index.tolist()\n        articles_metadata.drop(article_df_ids, inplace=True)\n        customers_metadata.drop(article_df_ids, inplace=True)\n        \n        train_embeddings = self._col2numpy(\n            column=articles_metadata[\"embedding\"].tolist()\n        )\n        \n        # Fitting the model on the article vectors.\n        self._model.fit(train_embeddings)\n        \n        # Getting the vector of the Customer with ID `customer_id`.\n        customer_embedding = self._get_customer_field_value(\n            customer_id, field_name=\"embedding\"\n        )\n        customer_embedding = np.expand_dims(customer_embedding, 0)\n        # Here `customer_embedding` is a NumPy array with shape (1, 768).\n        \n        # Making a prediction.\n        predictions = self._model.kneighbors(\n            customer_embedding, \n            n_neighbors=topk,\n            return_distance=False\n        )[0]\n        \n        # Returning the respective article IDs, based on the predicted indices.\n        return articles_metadata.iloc[\n            predictions.tolist()\n        ][\"article_id\"].tolist()\n        \n    def _col2numpy(self, column: List[np.ndarray]) -> np.ndarray:\n        # Stacking the list of NumPy arrays on the row axis.\n        array = np.stack(column, axis=0)\n        \n        return array\n    \n    def _get_customer_field_value(self, customer_id: str, field_name: str) -> Any:\n        return self._customers_metadata[\n             self._customers_metadata[\"customer_id\"] == customer_id\n        ][field_name].tolist()[0]\n\n\n# Selecting an arbitrary customer.\ncustomer_id = customer_embeddings[\"customer_id\"][42]\n    \nmetadata = SystemMetadata(\n    articles_metadata=embedded_articles,\n    customers_metadata=customer_embeddings\n)\n# Making a recommendation\narticle_recommender = ArticleRecommender(metadata)\nrecommended_articles = article_recommender.recommend(\n    customer_id=customer_id\n)\nprint(f\"Recommended articles for customer with ID '{customer_id}':\\n{recommended_articles}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_submission(system: ArticleRecommender, customer_ids: List[str]) -> pd.DataFrame:\n    recommendations = []\n    progressbar = tqdm(customer_ids)\n    \n    for i, customer_id in enumerate(progressbar):\n        progressbar.set_description(f\"Customer {i + 1}/{len(customer_ids)}\")\n        current_recommendations = system.recommend(\n            customer_id=customer_id\n        )\n        recommendations.append(\" \".join(current_recommendations))\n        \n    return pd.DataFrame.from_dict({\n        \"customer_id\": customer_ids,\n        \"prediction\": recommendations,\n    })\n\n\n# Generating a submission for a small subset of all Customers, just as an example.\nsubmission_df = generate_submission(\n    system=article_recommender,\n    customer_ids=customer_embeddings[\"customer_id\"].tolist()\n)\nsubmission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission_1.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"rec_repur\"></a>\n## 4.2. Recommend Items Repurchased","metadata":{}},{"cell_type":"code","source":"transactions_df['t_dat'] = pd.to_datetime(transactions_df['t_dat'])\ntransactions_df['month'] = transactions_df['t_dat'].dt.month","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Considering the predicted purchase time is Sep, Only take 6,7,8,9,10,11,12 month into consideration. \n#Using last week of final purchase as validation.\noriginal_transactions = transactions_df.loc[transactions_df['month'] >= 6]\ntransactions_df = original_transactions.loc[transactions_df['t_dat']<datetime(2020, 9, 7)]\n#valid_transactions = original_data.loc[transactions['t_dat']>=datetime(2020, 9, 7)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pursue a dict the key is customer_id the value is also a dict of the article and corresponding purchase times.\ndef create_dict(transactions_df,purchase_dict):\n    for i,x in enumerate(zip(transactions_df['customer_id'], transactions_df['article_id'])):\n        cust_id, art_id = x\n        if cust_id not in purchase_dict:\n            purchase_dict[cust_id] = {}\n    \n        if art_id not in purchase_dict[cust_id]:\n            purchase_dict[cust_id][art_id] = 0\n    \n        purchase_dict[cust_id][art_id] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Avoid Out of Memory\nn_split_prediction = 1000\npurchase_dict = {}\nn_chunk = (len(transactions_df) + n_split_prediction - 1)// n_split_prediction\nfor i in range(0, len(transactions_df), n_chunk):\n    #print(f\"chunk: {i}\")\n    \n    target_transactions = transactions_df.iloc[i:i+n_chunk]\n    create_dict(target_transactions,purchase_dict)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Caculate top12 items to make up for the absent position of previous purchase\ntop12 = list(transactions_df[\"article_id\"].value_counts().index[:12])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = submission_df[[\"customer_id\"]]\nstring_top12 = ' '.join(map(str, top12))\n\ndef generate_prediction(submission_df, purchase_dict, prediction_list, top12, string_top12):\n    for i, cust_id in enumerate(submission_df['customer_id'].values):\n        if cust_id in purchase_dict:\n            l = sorted(purchase_dict[cust_id].items(), key=lambda x: x[1], reverse=True)\n            l = [str(y[0]) for y in l]  # Convert integers to strings\n            if len(l) > 12:\n                s = ' '.join(l[:12])\n            else:\n                s = ' '.join(l + list(map(str, top12[:(12 - len(l))])))\n        else:\n            s = string_top12\n        prediction_list.append(s)\n    return prediction_list\n\nsub[\"prediction\"] = generate_prediction(submission_df, purchase_dict, [], top12, string_top12)\nprint(sub.head())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission_2.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"trend\"></a>\n## 4.3. Trending Products Weekly","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()\nN = 12","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions_df['article_id'] = transactions_df['article_id'].astype(str)\n\ntransactions_df['t_dat'] = pd.to_datetime(transactions_df['t_dat'])\nlast_ts = transactions_df['t_dat'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions_df['ldbw'] = transactions_df['t_dat'].progress_apply(lambda d: last_ts - (last_ts - d).floor('7D'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_sales = transactions_df.drop('customer_id', axis=1).groupby(['ldbw', 'article_id']).count()\nweekly_sales = weekly_sales.rename(columns={'t_dat': 'count'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_columns = ['t_dat', 'customer_id', 'article_id', 'ldbw']\ntransactions_df = transactions_df[selected_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions_df = transactions_df.join(weekly_sales, on=['ldbw', 'article_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_sales = weekly_sales.reset_index().set_index('article_id')\nlast_day = last_ts.strftime('%Y-%m-%d')\n\ntransactions_df = transactions_df.join(\n    weekly_sales.loc[weekly_sales['ldbw']==last_day, ['count']],\n    on='article_id', rsuffix=\"_targ\")\n\ntransactions_df['count_targ'].fillna(0, inplace=True)\ndel weekly_sales","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions_df['quotient'] = transactions_df['count_targ'] / transactions_df['count']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_sales = transactions_df.drop('customer_id', axis=1).groupby('article_id')['quotient'].sum()\ngeneral_pred = target_sales.nlargest(N).index.tolist()\ndel target_sales","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"purchase_dict = {}\n\nfor i in tqdm(transactions_df.index):\n    cust_id = transactions_df.at[i, 'customer_id']\n    art_id = transactions_df.at[i, 'article_id']\n    t_dat = transactions_df.at[i, 't_dat']\n\n    if cust_id not in purchase_dict:\n        purchase_dict[cust_id] = {}\n\n    if art_id not in purchase_dict[cust_id]:\n        purchase_dict[cust_id][art_id] = 0\n    \n    x = max(1, (last_ts - t_dat).days)\n\n    a, b, c, d = 2.5e4, 1.5e5, 2e-1, 1e3\n    y = a / np.sqrt(x) + b * np.exp(-c*x) - d\n\n    value = transactions_df.at[i, 'quotient'] * max(0, y)\n    purchase_dict[cust_id][art_id] += value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_list = []\nfor cust_id in tqdm(sub['customer_id']):\n    if cust_id in purchase_dict:\n        series = pd.Series(purchase_dict[cust_id])\n        series = series[series > 0]\n        l = series.nlargest(N).index.tolist()\n        if len(l) < N:\n            l = l + general_pred[:(N-len(l))]\n    else:\n        l = general_pred\n    pred_list.append(' '.join(l))\n\nsubmission_df['prediction'] = pred_list\nsubmission_df.to_csv('submission_3.csv', index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nsub0 = pd.read_csv('/kaggle/working/submission_1.csv').sort_values('customer_id').reset_index(drop=True)                                             # 0.0231\nsub1 = pd.read_csv('/kaggle/working/submission_2.csv').sort_values('customer_id').reset_index(drop=True)                # 0.0225\nsub2 = pd.read_csv('/kaggle/working/submission_3.csv').sort_values('customer_id').reset_index(drop=True)   \n\nsub0.columns = ['customer_id', 'prediction0']\nsub0['prediction1'] = sub1['prediction']\nsub0['prediction2'] = sub2['prediction']\n\ndel sub1, sub2\ngc.collect()\nsub0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cust_blend(dt, W = [1,1,1]):\n    #Global ensemble weights\n    #W = []\n\n    #Create a list of all model predictions\n    REC = []\n\n    # Second Try\n    REC.append(dt['prediction0'].split())\n    REC.append(dt['prediction1'].split())\n    REC.append(dt['prediction2'].split())\n    #Create a dictionary of items recommended.\n    #Assign a weight according the order of appearance and multiply by global weights\n    res = {}\n    for M in range(len(REC)):\n        for n, v in enumerate(REC[M]):\n            if v in res:\n                res[v] += (W[M]/(n+1))\n            else:\n                res[v] = (W[M]/(n+1))\n\n    # Sort dictionary by item weights\n    res = list(dict(sorted(res.items(), key=lambda item: -item[1])).keys())\n\n    # Return the top 12 items only\n    return ' '.join(res[:12])\n\nsub0['prediction'] = sub0.apply(cust_blend, W = [1.05,0.78,1.17], axis=1)\nsub0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del sub0['prediction0']\ndel sub0['prediction1']\ndel sub0['prediction2']\ngc.collect()\n\n\nsub0.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVD","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/mayukh18/reco","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:00:57.326260Z","iopub.execute_input":"2024-01-10T07:00:57.326552Z","iopub.status.idle":"2024-01-10T07:01:28.571986Z","shell.execute_reply.started":"2024-01-10T07:00:57.326526Z","shell.execute_reply":"2024-01-10T07:01:28.570700Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/mayukh18/reco\n  Cloning https://github.com/mayukh18/reco to /tmp/pip-req-build-dgilc6t7\n  Running command git clone --filter=blob:none --quiet https://github.com/mayukh18/reco /tmp/pip-req-build-dgilc6t7\n  Resolved https://github.com/mayukh18/reco to commit 3a61898cd5bb7c980732090a10e49e9d8c786a99\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from reco==0.2.1) (1.24.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from reco==0.2.1) (2.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->reco==0.2.1) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->reco==0.2.1) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->reco==0.2.1) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->reco==0.2.1) (1.16.0)\nBuilding wheels for collected packages: reco\n  Building wheel for reco (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for reco: filename=reco-0.2.1-cp310-cp310-linux_x86_64.whl size=9383703 sha256=b9c80e263f269c9cf354cbb47e3d549b6d55eaec599657f4785171be1703b9f9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-s5_c348n/wheels/08/84/1f/4f54fb9df6f7483c6d24d46bca75446401623e035556af4c70\nSuccessfully built reco\nInstalling collected packages: reco\nSuccessfully installed reco-0.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\", dtype={'article_id':str})\ndata[\"t_dat\"] = pd.to_datetime(data[\"t_dat\"])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:02:29.363848Z","iopub.execute_input":"2024-01-10T07:02:29.364386Z","iopub.status.idle":"2024-01-10T07:03:43.397787Z","shell.execute_reply.started":"2024-01-10T07:02:29.364353Z","shell.execute_reply":"2024-01-10T07:03:43.396734Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       t_dat                                        customer_id  article_id  \\\n0 2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n1 2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n2 2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n3 2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n4 2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n\n      price  sales_channel_id  \n0  0.050831                 2  \n1  0.030492                 2  \n2  0.015237                 2  \n3  0.016932                 2  \n4  0.016932                 2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-09-20</td>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>0663713001</td>\n      <td>0.050831</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-09-20</td>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>0541518023</td>\n      <td>0.030492</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-09-20</td>\n      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n      <td>0505221004</td>\n      <td>0.015237</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-09-20</td>\n      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n      <td>0685687003</td>\n      <td>0.016932</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-09-20</td>\n      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n      <td>0685687004</td>\n      <td>0.016932</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"All Transactions Date Range: {} to {}\".format(data['t_dat'].min(), data['t_dat'].max()))\n\ndata[\"t_dat\"] = pd.to_datetime(data[\"t_dat\"])\ntrain1 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,8)) & (data['t_dat'] < datetime.datetime(2020,9,16))]\ntrain2 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,1)) & (data['t_dat'] < datetime.datetime(2020,9,8))]\ntrain3 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,8,23)) & (data['t_dat'] < datetime.datetime(2020,9,1))]\ntrain4 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,8,15)) & (data['t_dat'] < datetime.datetime(2020,8,23))]\n\nval = data.loc[data[\"t_dat\"] >= datetime.datetime(2020,9,16)]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:03:43.399753Z","iopub.execute_input":"2024-01-10T07:03:43.400131Z","iopub.status.idle":"2024-01-10T07:03:45.109327Z","shell.execute_reply.started":"2024-01-10T07:03:43.400097Z","shell.execute_reply":"2024-01-10T07:03:45.108363Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"All Transactions Date Range: 2018-09-20 00:00:00 to 2020-09-22 00:00:00\n","output_type":"stream"}]},{"cell_type":"code","source":"# List of all purchases per user (has repetitions)\npositive_items_per_user1 = train1.groupby(['customer_id'])['article_id'].apply(list)\npositive_items_per_user2 = train2.groupby(['customer_id'])['article_id'].apply(list)\npositive_items_per_user3 = train3.groupby(['customer_id'])['article_id'].apply(list)\npositive_items_per_user4 = train4.groupby(['customer_id'])['article_id'].apply(list)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:03:45.110558Z","iopub.execute_input":"2024-01-10T07:03:45.110834Z","iopub.status.idle":"2024-01-10T07:03:55.714219Z","shell.execute_reply.started":"2024-01-10T07:03:45.110810Z","shell.execute_reply":"2024-01-10T07:03:55.713275Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train1, train2, train3, train4], axis=0)\n\n#time decay popularity of each article\ntrain['pop_factor'] = train['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,16) - x).days**2)\npopular_items_group = train.groupby(['article_id'])['pop_factor'].sum()\n\n# purchase count of each article\nitems_total_count = train.groupby(['article_id'])['article_id'].count()\n# purchase count of each user\nusers_total_count = train.groupby(['customer_id'])['customer_id'].count()\n\n\ntrain['feedback'] = 1\ntrain = train.groupby(['customer_id', 'article_id']).sum().reset_index()\ntrain['feedback'] = train.apply(lambda row: row['feedback']/popular_items_group[row['article_id']], axis=1)\n\ntrain['feedback'] = train['feedback'].apply(lambda x: 5.0 if x>5.0 else x)\ntrain.drop(['price', 'sales_channel_id'], axis=1, inplace=True)\n\n# shuffling\ntrain = train.sample(frac=1).reset_index(drop=True)\ntrain['feedback'].describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:05:04.009219Z","iopub.execute_input":"2024-01-10T07:05:04.009620Z","iopub.status.idle":"2024-01-10T07:05:19.652071Z","shell.execute_reply.started":"2024-01-10T07:05:04.009588Z","shell.execute_reply":"2024-01-10T07:05:19.650524Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m users_total_count \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     13\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 14\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     15\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m/\u001b[39mpopular_items_group[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m5.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m5.0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:3063\u001b[0m, in \u001b[0;36mGroupBy.sum\u001b[0;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   3058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3059\u001b[0m     \u001b[38;5;66;03m# If we are grouping on categoricals we want unobserved categories to\u001b[39;00m\n\u001b[1;32m   3060\u001b[0m     \u001b[38;5;66;03m# return zero, rather than the default of NaN which the reindexing in\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m     \u001b[38;5;66;03m# _agg_general() returns. GH #31422\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 3063\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m            \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnpfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_output(result, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1839\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[0;34m(self, numeric_only, min_count, alias, npfunc)\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_agg_general\u001b[39m(\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     npfunc: Callable,\n\u001b[1;32m   1838\u001b[0m ):\n\u001b[0;32m-> 1839\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1929\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1929\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1930\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   1931\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:1431\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1429\u001b[0m             result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1431\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_blocks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:366\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1905\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray_func\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1908\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1914\u001b[0m         \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m         \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m         \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m         \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[1;32m   1918\u001b[0m         \u001b[38;5;66;03m# TODO: avoid special casing SparseArray here\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, SparseArray):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/ops.py:816\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[0;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m ids, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_info\n\u001b[1;32m    815\u001b[0m ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups\n\u001b[0;32m--> 816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcy_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcython_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomp_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/ops.py:526\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[0;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_axis(axis, values)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_groupby_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_dropped_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_dropped_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_op_ndim_compat(\n\u001b[1;32m    536\u001b[0m     values,\n\u001b[1;32m    537\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    542\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1637\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._groupby_op\u001b[0;34m(self, how, has_dropped_na, min_count, ngroups, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;66;03m# Adding/multiplying datetimes is not valid\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumsum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumprod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskew\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 1637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime64 type does not support \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m operations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1639\u001b[0m         \u001b[38;5;66;03m# GH#34479\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1641\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with datetime64 dtypes is deprecated and will raise in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1642\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture version. Use (obj != pd.Timestamp(0)).\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1643\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1644\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1645\u001b[0m         )\n","\u001b[0;31mTypeError\u001b[0m: datetime64 type does not support sum operations"],"ename":"TypeError","evalue":"datetime64 type does not support sum operations","output_type":"error"}]},{"cell_type":"code","source":"train_pop = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,1)) & (data['t_dat'] < datetime.datetime(2020,9,16))]\ntrain_pop['pop_factor'] = train_pop['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,16) - x).days)\npopular_items_group = train_pop.groupby(['article_id'])['pop_factor'].sum()\n\n_, popular_items = zip(*sorted(zip(popular_items_group, popular_items_group.keys()))[::-1])\n\ntrain_pop['pop_factor'].describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:04:12.699091Z","iopub.status.idle":"2024-01-10T07:04:12.699587Z","shell.execute_reply.started":"2024-01-10T07:04:12.699320Z","shell.execute_reply":"2024-01-10T07:04:12.699343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_most_freq_next_item(user_group):\n    next_items = {}\n    for user in tqdm(user_group.keys()):\n        items = user_group[user]\n        for i,item in enumerate(items[:-1]):\n            if item not in next_items:\n                next_items[item] = []\n            if item != items[i+1]:\n                next_items[item].append(items[i+1])\n\n    pred_next = {}\n    for item in next_items:\n        if len(next_items[item]) >= 5:\n            most_common = Counter(next_items[item]).most_common()\n            ratio = most_common[0][1]/len(next_items[item])\n            if ratio >= 0.1:\n                pred_next[item] = most_common[0][0]\n            \n    return pred_next\n\nuser_group = train.groupby(['customer_id'])['article_id'].apply(list)\npred_next = get_most_freq_next_item(user_group)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:04:12.701425Z","iopub.status.idle":"2024-01-10T07:04:12.701805Z","shell.execute_reply.started":"2024-01-10T07:04:12.701634Z","shell.execute_reply":"2024-01-10T07:04:12.701650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from reco.recommender import FunkSVD\nfrom reco.metrics import rmse\n\n# k = number of dimensions of the latent embedding. formatizer dict takes in names of the columns\n# for user, item and values/feedback/ratings respectively.\n\nsvd = FunkSVD(k=8, learning_rate=0.008, regularizer = .01, iterations = 80, method = 'stochastic', bias=True)\nsvd.fit(X=train, formatizer={'user':'customer_id', 'item':'article_id', 'value':'feedback'},verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:04:12.703013Z","iopub.status.idle":"2024-01-10T07:04:12.703492Z","shell.execute_reply.started":"2024-01-10T07:04:12.703234Z","shell.execute_reply":"2024-01-10T07:04:12.703256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apk(actual, predicted, k=12):\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=12):\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:04:12.705313Z","iopub.status.idle":"2024-01-10T07:04:12.705776Z","shell.execute_reply.started":"2024-01-10T07:04:12.705546Z","shell.execute_reply":"2024-01-10T07:04:12.705567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_items_val = val.groupby(['customer_id'])['article_id'].apply(list)\nval_users = positive_items_val.keys()\nval_items = []\n\nfor i,user in tqdm(enumerate(val_users)):\n    val_items.append(positive_items_val[user])\n    \nprint(\"Total users in validation:\", len(val_users))","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:04:12.707600Z","iopub.status.idle":"2024-01-10T07:04:12.708070Z","shell.execute_reply.started":"2024-01-10T07:04:12.707820Z","shell.execute_reply":"2024-01-10T07:04:12.707852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\noutputs = []\ncnt = 0\n\npopular_items = list(popular_items)\n\nfor user in tqdm(val_users):\n    user_output = []\n    if user in positive_items_per_user1.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user1[user]).most_common()}\n        user_output += list(most_common_items_of_user.keys())[:12]\n    if user in positive_items_per_user2.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user2[user]).most_common()}\n        user_output += list(most_common_items_of_user.keys())[:12]\n    if user in positive_items_per_user3.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user3[user]).most_common()}\n        user_output += list(most_common_items_of_user.keys())[:12]\n    if user in positive_items_per_user4.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user4[user]).most_common()}\n        user_output += list(most_common_items_of_user.keys())[:12]\n    \n    user_output += [pred_next[item] for item in user_output if item in pred_next and pred_next[item] not in user_output]      \n    \n    user_output += list(popular_items[:12 - len(user_output)])\n    outputs.append(user_output)\n    \nprint(\"mAP Score on Validation set:\", mapk(val_items, outputs))","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:04:12.709490Z","iopub.status.idle":"2024-01-10T07:04:12.709818Z","shell.execute_reply.started":"2024-01-10T07:04:12.709659Z","shell.execute_reply":"2024-01-10T07:04:12.709674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\noutputs = []\ncnt = 0\n\npopular_items = list(popular_items)\nuserindexes = {svd.users[i]:i for i in range(len(svd.users))}\n\nfor user in tqdm(val_users):\n    user_output = []\n    if user in positive_items_per_user1.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user1[user]).most_common()}\n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = svd.items.index(k)\n                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    if user in positive_items_per_user2.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user2[user]).most_common()}\n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = svd.items.index(k)\n                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    if user in positive_items_per_user3.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user3[user]).most_common()}\n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = svd.items.index(k)\n                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    if user in positive_items_per_user4.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user4[user]).most_common()}\n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = svd.items.index(k)\n                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    user_output += [pred_next[item] for item in user_output if item in pred_next and pred_next[item] not in user_output]      \n    \n    user_output += list(popular_items[:12 - len(user_output)])\n    outputs.append(user_output)\n    \nprint(\"mAP Score on Validation set:\", mapk(val_items, outputs))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = []\ncnt = 0\n\npopular_items = list(popular_items)\nuserindexes = {f.users[i]:i for i in range(len(f.users))}\n\nfor user in tqdm(submission['customer_id']):\n    user_output = []\n    if user in positive_items_per_user1.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user1[user]).most_common()}\n        \n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = f.items.index(k)\n                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    if user in positive_items_per_user2.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user2[user]).most_common()}\n        \n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = f.items.index(k)\n                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    if user in positive_items_per_user3.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user3[user]).most_common()}\n        \n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = f.items.index(k)\n                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    if user in positive_items_per_user4.keys():\n        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user4[user]).most_common()}\n        \n        user_index = userindexes[user]\n        new_order = {}\n        for k in list(most_common_items_of_user.keys())[:20]:\n            try:\n                itemindex = f.items.index(k)\n                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n            except:\n                pred_value = most_common_items_of_user[k]\n            new_order[k] = pred_value\n        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n        \n    user_output += [pred_next[item] for item in user_output if item in pred_next and pred_next[item] not in user_output]      \n    \n    user_output += list(popular_items[:12 - len(user_output)])\n    outputs.append(user_output)\n    \nstr_outputs = []\nfor output in outputs:\n    str_outputs.append(\" \".join([str(x) for x in output]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,16)) & (data['t_dat'] < datetime.datetime(2020,9,23))]\ntrain2 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,8)) & (data['t_dat'] < datetime.datetime(2020,9,16))]\ntrain3 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,8,31)) & (data['t_dat'] < datetime.datetime(2020,9,8))]\ntrain4 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,8,23)) & (data['t_dat'] < datetime.datetime(2020,8,31))]\n\npositive_items_per_user1 = train1.groupby(['customer_id'])['article_id'].apply(list)\npositive_items_per_user2 = train2.groupby(['customer_id'])['article_id'].apply(list)\npositive_items_per_user3 = train3.groupby(['customer_id'])['article_id'].apply(list)\npositive_items_per_user4 = train4.groupby(['customer_id'])['article_id'].apply(list)\n\ntrain = pd.concat([train1, train2], axis=0)\ntrain['pop_factor'] = train['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days)\npopular_items_group = train.groupby(['article_id'])['pop_factor'].sum()\n\n_, popular_items = zip(*sorted(zip(popular_items_group, popular_items_group.keys()))[::-1])\n\nuser_group = pd.concat([train1, train2, train3, train4], axis=0).groupby(['customer_id'])['article_id'].apply(list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['prediction'] = str_outputs\nsubmission.to_csv(\"submissions.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"rerank\"></a>\n# 5. Rerank","metadata":{}},{"cell_type":"code","source":"from lightgbm.sklearn import LGBMRanker\nfrom datetime import timedelta\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom typing import List, Union, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom transformers import AutoTokenizer, AutoModel\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:07:29.755329Z","iopub.execute_input":"2024-01-10T07:07:29.755704Z","iopub.status.idle":"2024-01-10T07:07:32.505505Z","shell.execute_reply.started":"2024-01-10T07:07:29.755674Z","shell.execute_reply":"2024-01-10T07:07:32.504726Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"user_features = pd.read_parquet('../input/ranking-features/user_features.parquet')\nitem_features = pd.read_parquet('../input/ranking-features/item_features.parquet')\ntransactions_df = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\ntransactions_df.t_dat = pd.to_datetime( transactions_df.t_dat )","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:07:32.507090Z","iopub.execute_input":"2024-01-10T07:07:32.507384Z","iopub.status.idle":"2024-01-10T07:08:18.847129Z","shell.execute_reply.started":"2024-01-10T07:07:32.507356Z","shell.execute_reply":"2024-01-10T07:08:18.846288Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_4w = transactions_df[transactions_df['t_dat'] >= pd.to_datetime('2020-08-24')].copy()\ndf_3w = transactions_df[transactions_df['t_dat'] >= pd.to_datetime('2020-08-31')].copy()\ndf_2w = transactions_df[transactions_df['t_dat'] >= pd.to_datetime('2020-09-07')].copy()\ndf_1w = transactions_df[transactions_df['t_dat'] >= pd.to_datetime('2020-09-15')].copy()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:08:18.848255Z","iopub.execute_input":"2024-01-10T07:08:18.848582Z","iopub.status.idle":"2024-01-10T07:08:19.310840Z","shell.execute_reply.started":"2024-01-10T07:08:18.848554Z","shell.execute_reply":"2024-01-10T07:08:19.310012Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"user_features[['club_member_status', 'fashion_news_frequency']] = (\n                   user_features[['club_member_status', 'fashion_news_frequency']]\n                   .apply(lambda x: pd.factorize(x)[0])\n).astype('int8')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:08:19.312755Z","iopub.execute_input":"2024-01-10T07:08:19.313053Z","iopub.status.idle":"2024-01-10T07:08:19.555437Z","shell.execute_reply.started":"2024-01-10T07:08:19.313028Z","shell.execute_reply":"2024-01-10T07:08:19.554655Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"transactions_df = (\n    transactions_df\n    .merge(user_features, on = ('customer_id'))\n    .merge(item_features, on = ('article_id'))\n)\ntransactions_df.sort_values(['t_dat', 'customer_id'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:08:19.556562Z","iopub.execute_input":"2024-01-10T07:08:19.556844Z","iopub.status.idle":"2024-01-10T07:10:05.965237Z","shell.execute_reply.started":"2024-01-10T07:08:19.556819Z","shell.execute_reply":"2024-01-10T07:10:05.964155Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#for simplicity let's take only 1M rows\nN_ROWS = 1_000_000\n\ntrain = transactions_df.loc[ transactions_df.t_dat <= pd.to_datetime('2020-09-15') ].iloc[:N_ROWS]\nvalid = transactions_df.loc[ transactions_df.t_dat >= pd.to_datetime('2020-09-16') ]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:05.966557Z","iopub.execute_input":"2024-01-10T07:10:05.966872Z","iopub.status.idle":"2024-01-10T07:10:11.986641Z","shell.execute_reply.started":"2024-01-10T07:10:05.966845Z","shell.execute_reply":"2024-01-10T07:10:11.985636Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#delete transactions to save memory\ndel transactions_df","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:11.987891Z","iopub.execute_input":"2024-01-10T07:10:11.988203Z","iopub.status.idle":"2024-01-10T07:10:12.184957Z","shell.execute_reply.started":"2024-01-10T07:10:11.988175Z","shell.execute_reply":"2024-01-10T07:10:12.183763Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train.shape, valid.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:12.186610Z","iopub.execute_input":"2024-01-10T07:10:12.186930Z","iopub.status.idle":"2024-01-10T07:10:12.199748Z","shell.execute_reply.started":"2024-01-10T07:10:12.186900Z","shell.execute_reply":"2024-01-10T07:10:12.198862Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"((1000000, 88), (240311, 88))"},"metadata":{}}]},{"cell_type":"code","source":"purchase_dict_4w = {}\n\nfor i,x in enumerate(zip(df_4w['customer_id'], df_4w['article_id'])):\n    cust_id, art_id = x\n    if cust_id not in purchase_dict_4w:\n        purchase_dict_4w[cust_id] = {}\n    \n    if art_id not in purchase_dict_4w[cust_id]:\n        purchase_dict_4w[cust_id][art_id] = 0\n    \n    purchase_dict_4w[cust_id][art_id] += 1\n\ndummy_list_4w = list((df_4w['article_id'].value_counts()).index)[:12]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:12.202479Z","iopub.execute_input":"2024-01-10T07:10:12.202850Z","iopub.status.idle":"2024-01-10T07:10:13.814480Z","shell.execute_reply.started":"2024-01-10T07:10:12.202812Z","shell.execute_reply":"2024-01-10T07:10:13.813692Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"purchase_dict_3w = {}\n\nfor i,x in enumerate(zip(df_3w['customer_id'], df_3w['article_id'])):\n    cust_id, art_id = x\n    if cust_id not in purchase_dict_3w:\n        purchase_dict_3w[cust_id] = {}\n    \n    if art_id not in purchase_dict_3w[cust_id]:\n        purchase_dict_3w[cust_id][art_id] = 0\n    \n    purchase_dict_3w[cust_id][art_id] += 1\n\ndummy_list_3w = list((df_3w['article_id'].value_counts()).index)[:12]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:13.815563Z","iopub.execute_input":"2024-01-10T07:10:13.815865Z","iopub.status.idle":"2024-01-10T07:10:15.005362Z","shell.execute_reply.started":"2024-01-10T07:10:13.815839Z","shell.execute_reply":"2024-01-10T07:10:15.004387Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"purchase_dict_2w = {}\n\nfor i,x in enumerate(zip(df_2w['customer_id'], df_2w['article_id'])):\n    cust_id, art_id = x\n    if cust_id not in purchase_dict_2w:\n        purchase_dict_2w[cust_id] = {}\n    \n    if art_id not in purchase_dict_2w[cust_id]:\n        purchase_dict_2w[cust_id][art_id] = 0\n    \n    purchase_dict_2w[cust_id][art_id] += 1\n\ndummy_list_2w = list((df_2w['article_id'].value_counts()).index)[:12]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:15.006714Z","iopub.execute_input":"2024-01-10T07:10:15.007012Z","iopub.status.idle":"2024-01-10T07:10:15.872817Z","shell.execute_reply.started":"2024-01-10T07:10:15.006986Z","shell.execute_reply":"2024-01-10T07:10:15.871807Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"purchase_dict_1w = {}\n\nfor i,x in enumerate(zip(df_1w['customer_id'], df_1w['article_id'])):\n    cust_id, art_id = x\n    if cust_id not in purchase_dict_1w:\n        purchase_dict_1w[cust_id] = {}\n    \n    if art_id not in purchase_dict_1w[cust_id]:\n        purchase_dict_1w[cust_id][art_id] = 0\n    \n    purchase_dict_1w[cust_id][art_id] += 1\n\ndummy_list_1w = list((df_1w['article_id'].value_counts()).index)[:12]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:15.874411Z","iopub.execute_input":"2024-01-10T07:10:15.875083Z","iopub.status.idle":"2024-01-10T07:10:16.260198Z","shell.execute_reply.started":"2024-01-10T07:10:15.875045Z","shell.execute_reply":"2024-01-10T07:10:16.259153Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def prepare_candidates(customers_id, n_candidates = 12):\n  \"\"\"\n  df - basically, dataframe with customers(customers should be unique)\n  \"\"\"\n  prediction_dict = {}\n  dummy_list = list((df_2w['article_id'].value_counts()).index)[:n_candidates]\n\n  for i, cust_id in tqdm(enumerate(customers_id)):\n    # comment this for validation\n    if cust_id in purchase_dict_1w:\n        l = sorted((purchase_dict_1w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n        l = [y[0] for y in l]\n        if len(l)>n_candidates:\n            s = l[:n_candidates]\n        else:\n            s = l+dummy_list_1w[:(n_candidates-len(l))]\n    elif cust_id in purchase_dict_2w:\n        l = sorted((purchase_dict_2w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n        l = [y[0] for y in l]\n        if len(l)>n_candidates:\n            s = l[:n_candidates]\n        else:\n            s = l+dummy_list_2w[:(n_candidates-len(l))]\n    elif cust_id in purchase_dict_3w:\n        l = sorted((purchase_dict_3w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n        l = [y[0] for y in l]\n        if len(l)>n_candidates:\n            s = l[:n_candidates]\n        else:\n            s = l+dummy_list_3w[:(n_candidates-len(l))]\n    elif cust_id in purchase_dict_4w:\n        l = sorted((purchase_dict_4w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n        l = [y[0] for y in l]\n        if len(l)>n_candidates:\n            s = l[:n_candidates]\n        else:\n            s = l+dummy_list_4w[:(n_candidates-len(l))]\n    else:\n        s = dummy_list\n    prediction_dict[cust_id] = s\n\n  k = list(map(lambda x: x[0], prediction_dict.items()))\n  v = list(map(lambda x: x[1], prediction_dict.items()))\n  negatives_df = pd.DataFrame({'customer_id': k, 'negatives': v})\n  negatives_df = (\n      negatives_df\n      .explode('negatives')\n      .rename(columns = {'negatives': 'article_id'})\n  )\n  return negatives_df","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:16.261585Z","iopub.execute_input":"2024-01-10T07:10:16.261879Z","iopub.status.idle":"2024-01-10T07:10:16.275317Z","shell.execute_reply.started":"2024-01-10T07:10:16.261853Z","shell.execute_reply":"2024-01-10T07:10:16.274359Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#take only last 15 transactions\ntrain['rank'] = range(len(train))\ntrain = (\n    train\n    .assign(\n        rn = train.groupby(['customer_id'])['rank']\n                  .rank(method='first', ascending=False))\n    .query(\"rn <= 15\")\n    .drop(columns = ['price', 'sales_channel_id'])\n    .sort_values(['t_dat', 'customer_id'])\n)\ntrain['label'] = 1\n\ndel train['rank']\ndel train['rn']\n\nvalid.sort_values(['t_dat', 'customer_id'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:16.276373Z","iopub.execute_input":"2024-01-10T07:10:16.276675Z","iopub.status.idle":"2024-01-10T07:10:18.406914Z","shell.execute_reply.started":"2024-01-10T07:10:16.276650Z","shell.execute_reply":"2024-01-10T07:10:18.405895Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"last_dates = (\n    train\n    .groupby('customer_id')['t_dat']\n    .max()\n    .to_dict()\n)\n\nnegatives = prepare_candidates(train['customer_id'].unique(), 15)\nnegatives['t_dat'] = negatives['customer_id'].map(last_dates)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:18.408193Z","iopub.execute_input":"2024-01-10T07:10:18.408530Z","iopub.status.idle":"2024-01-10T07:10:22.312950Z","shell.execute_reply.started":"2024-01-10T07:10:18.408502Z","shell.execute_reply":"2024-01-10T07:10:22.311961Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d44e273c594367be83832a2767f3b7"}},"metadata":{}}]},{"cell_type":"code","source":"# negatives = negatives.drop('t_dat', axis=1)\nnegatives","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:10:22.314377Z","iopub.execute_input":"2024-01-10T07:10:22.314914Z","iopub.status.idle":"2024-01-10T07:10:22.327389Z","shell.execute_reply.started":"2024-01-10T07:10:22.314886Z","shell.execute_reply":"2024-01-10T07:10:22.326503Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                              customer_id article_id  \\\n0       000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  794321007   \n0       000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  924243001   \n0       000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  924243002   \n0       000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  923758001   \n0       000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  918522001   \n...                                                   ...        ...   \n214935  461802adf572dddc90bf68e441db0039809338e20768d7...  751471043   \n214935  461802adf572dddc90bf68e441db0039809338e20768d7...  924243002   \n214935  461802adf572dddc90bf68e441db0039809338e20768d7...  706016001   \n214935  461802adf572dddc90bf68e441db0039809338e20768d7...  850917001   \n214935  461802adf572dddc90bf68e441db0039809338e20768d7...  673677002   \n\n            t_dat  \n0      2018-09-24  \n0      2018-09-24  \n0      2018-09-24  \n0      2018-09-24  \n0      2018-09-24  \n...           ...  \n214935 2018-10-11  \n214935 2018-10-11  \n214935 2018-10-11  \n214935 2018-10-11  \n214935 2018-10-11  \n\n[3171618 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>t_dat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>794321007</td>\n      <td>2018-09-24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>924243001</td>\n      <td>2018-09-24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>924243002</td>\n      <td>2018-09-24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>923758001</td>\n      <td>2018-09-24</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n      <td>918522001</td>\n      <td>2018-09-24</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>214935</th>\n      <td>461802adf572dddc90bf68e441db0039809338e20768d7...</td>\n      <td>751471043</td>\n      <td>2018-10-11</td>\n    </tr>\n    <tr>\n      <th>214935</th>\n      <td>461802adf572dddc90bf68e441db0039809338e20768d7...</td>\n      <td>924243002</td>\n      <td>2018-10-11</td>\n    </tr>\n    <tr>\n      <th>214935</th>\n      <td>461802adf572dddc90bf68e441db0039809338e20768d7...</td>\n      <td>706016001</td>\n      <td>2018-10-11</td>\n    </tr>\n    <tr>\n      <th>214935</th>\n      <td>461802adf572dddc90bf68e441db0039809338e20768d7...</td>\n      <td>850917001</td>\n      <td>2018-10-11</td>\n    </tr>\n    <tr>\n      <th>214935</th>\n      <td>461802adf572dddc90bf68e441db0039809338e20768d7...</td>\n      <td>673677002</td>\n      <td>2018-10-11</td>\n    </tr>\n  </tbody>\n</table>\n<p>3171618 rows  3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_1 = pd.read_csv('/kaggle/input/submit/submission_1.csv')\nsubmission_2 = pd.read_csv('/kaggle/input/submit/submission_2 (1).csv')\nsubmission_3 = pd.read_csv('/kaggle/input/submit/submission_3 (1).csv')\n# submission_4 = pd.read_csv('/kaggle/input/submit/')\nmerged_sub = pd.merge(pd.merge(submission_1, submission_2, how='outer', on='customer_id'), submission_3, how='outer', on='customer_id')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_predictions(row):\n    return f\"{row['prediction_x']} {row['prediction_y']} {row['prediction']}\"\n\n# p dng hm cho mi hng ca DataFrame\nmerged_sub['combined_prediction'] = merged_sub.apply(combine_predictions, axis=1)\n\n# Convert combined_prediction column to a list of strings\nmerged_sub['combined_prediction'] = merged_sub['combined_prediction'].apply(lambda x: x.split())\n\n# Explode the list of strings in combined_prediction column\nmerged_sub = merged_sub.explode('combined_prediction')\n\ndel merged_sub['prediction_x']\ndel merged_sub['prediction_y']\ndel merged_sub['prediction']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_sub = merged_sub.iloc[:100000, :]\nmerged_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_list = merged_sub['customer_id'].unique()\nactual_purchases = train[train['customer_id'].isin(customer_list)][['customer_id', 'article_id', 'label']]\nactual_purchases = actual_purchases.groupby('customer_id')['article_id'].apply(list).reset_index(name='actual_purchases')\n\nmerged_sub['label'] = 0\n\n# Duyt qua tng dng trong actual_purchases\nfor index, row in actual_purchases.iterrows():\n    # Ly danh sch cc mua hng thc t ca khch hng hin ti\n    purchases = row['actual_purchases']\n    \n    # Lc cc dng trong merged_sub c customer_id trng khp v combined_prediction thuc purchases\n    mask = (merged_sub['customer_id'] == row['customer_id']) & (merged_sub['combined_prediction'].isin(purchases))\n    \n    # Gn nhn 1 cho cc dng tha mn iu kin\n    merged_sub.loc[mask, 'label'] = 1\n\n# Hin th DataFrame kt qu\nprint(merged_sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_sub = merged_sub.rename(columns={'combined_prediction': 'article_id'})\n\n# Gi s negatives l DataFrame cha thng tin v cc mt hng c label = 0\n# To DataFrame cha cc hng c label = 0 t merged_sub\nnegatives_from_merged_sub = merged_sub[merged_sub['label'] == 0]\n\n# Concatenate DataFrame negatives_from_merged_sub vi DataFrame negatives\nnegatives_combined = pd.concat([negatives, negatives_from_merged_sub[['customer_id', 'article_id']]], ignore_index=True)\n\n# Hin th DataFrame kt qu\nprint(negatives_combined)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negatives_combined = (\n    negatives_combined\n    .merge(user_features, on = ('customer_id'))\n    .merge(item_features, on = ('article_id'))\n)\nnegatives_combined['label'] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, negatives_combined])\ntrain.sort_values(['customer_id', 't_dat'], inplace = True)\ntrain_baskets = train.groupby(['customer_id'])['article_id'].count().values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.to_parquet('/kaggle/working/train_label.parquet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_parquet('/kaggle/input/train-label/train_label.parquet')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:07:04.484299Z","iopub.execute_input":"2024-01-10T07:07:04.485195Z","iopub.status.idle":"2024-01-10T07:07:06.629562Z","shell.execute_reply.started":"2024-01-10T07:07:04.485161Z","shell.execute_reply":"2024-01-10T07:07:06.628508Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:07:06.631420Z","iopub.execute_input":"2024-01-10T07:07:06.631761Z","iopub.status.idle":"2024-01-10T07:07:07.242765Z","shell.execute_reply.started":"2024-01-10T07:07:06.631733Z","shell.execute_reply":"2024-01-10T07:07:07.241851Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"              t_dat                                        customer_id  \\\n5785873  2018-09-21  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n11011079 2018-09-21  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n14930470 2018-09-21  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n1316293  2018-09-25  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n18915918 2018-09-25  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n...             ...                                                ...   \n2051405  2018-09-20  ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...   \n2243670  2018-09-20  ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...   \n3116205  2018-09-20  ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...   \n3125001  2018-09-20  ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...   \n3128091  2018-09-20  ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...   \n\n          article_id  mean_transactions  max_transactions  min_transactions  \\\n5785873    640244003           0.030255          0.084729          0.006763   \n11011079   639677008           0.030255          0.084729          0.006763   \n14930470   583558001           0.030255          0.084729          0.006763   \n1316293    666448006           0.030255          0.084729          0.006763   \n18915918   521269001           0.030255          0.084729          0.006763   \n...              ...                ...               ...               ...   \n2051405    918292001           0.017532          0.042356          0.004559   \n2243670    751471043           0.017532          0.042356          0.004559   \n3116205    794819001           0.017532          0.042356          0.004559   \n3125001    884081001           0.017532          0.042356          0.004559   \n3128091    689365050           0.017532          0.042356          0.004559   \n\n          median_transactions  sum_transactions  max_minus_min_transactions  \\\n5785873              0.025407          2.601932                    0.077966   \n11011079             0.025407          2.601932                    0.077966   \n14930470             0.025407          2.601932                    0.077966   \n1316293              0.025407          2.601932                    0.077966   \n18915918             0.025407          2.601932                    0.077966   \n...                       ...               ...                         ...   \n2051405              0.015237          0.788932                    0.037797   \n2243670              0.015237          0.788932                    0.037797   \n3116205              0.015237          0.788932                    0.037797   \n3125001              0.015237          0.788932                    0.037797   \n3128091              0.015237          0.788932                    0.037797   \n\n          n_transactions  ...  graphical_appearance_name_3  \\\n5785873               86  ...                            1   \n11011079              86  ...                            1   \n14930470              86  ...                            0   \n1316293               86  ...                            1   \n18915918              86  ...                            1   \n...                  ...  ...                          ...   \n2051405               45  ...                            1   \n2243670               45  ...                            0   \n3116205               45  ...                            1   \n3125001               45  ...                            0   \n3128091               45  ...                            1   \n\n          colour_group_name_3  perceived_colour_value_name_3  \\\n5785873                     1                              1   \n11011079                    0                              1   \n14930470                    0                              0   \n1316293                     0                              0   \n18915918                    1                              1   \n...                       ...                            ...   \n2051405                     1                              1   \n2243670                     1                              1   \n3116205                     1                              1   \n3125001                     1                              1   \n3128091                     0                              0   \n\n          perceived_colour_master_name_3  department_name_3  index_name_3  \\\n5785873                                1                  0             1   \n11011079                               1                  0             1   \n14930470                               1                  0             1   \n1316293                                1                  1             1   \n18915918                               1                  0             1   \n...                                  ...                ...           ...   \n2051405                                1                  0             0   \n2243670                                1                  1             1   \n3116205                                1                  1             1   \n3125001                                1                  1             1   \n3128091                                0                  0             1   \n\n          index_group_name_3  section_name_3  garment_group_name_3  label  \n5785873                    1               1                     0      1  \n11011079                   1               1                     0      1  \n14930470                   1               1                     0      1  \n1316293                    1               1                     0      1  \n18915918                   1               1                     0      1  \n...                      ...             ...                   ...    ...  \n2051405                    0               0                     1      0  \n2243670                    1               1                     0      0  \n3116205                    1               0                     0      0  \n3125001                    1               0                     1      0  \n3128091                    1               1                     0      0  \n\n[4108998 rows x 87 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>mean_transactions</th>\n      <th>max_transactions</th>\n      <th>min_transactions</th>\n      <th>median_transactions</th>\n      <th>sum_transactions</th>\n      <th>max_minus_min_transactions</th>\n      <th>n_transactions</th>\n      <th>...</th>\n      <th>graphical_appearance_name_3</th>\n      <th>colour_group_name_3</th>\n      <th>perceived_colour_value_name_3</th>\n      <th>perceived_colour_master_name_3</th>\n      <th>department_name_3</th>\n      <th>index_name_3</th>\n      <th>index_group_name_3</th>\n      <th>section_name_3</th>\n      <th>garment_group_name_3</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5785873</th>\n      <td>2018-09-21</td>\n      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n      <td>640244003</td>\n      <td>0.030255</td>\n      <td>0.084729</td>\n      <td>0.006763</td>\n      <td>0.025407</td>\n      <td>2.601932</td>\n      <td>0.077966</td>\n      <td>86</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11011079</th>\n      <td>2018-09-21</td>\n      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n      <td>639677008</td>\n      <td>0.030255</td>\n      <td>0.084729</td>\n      <td>0.006763</td>\n      <td>0.025407</td>\n      <td>2.601932</td>\n      <td>0.077966</td>\n      <td>86</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14930470</th>\n      <td>2018-09-21</td>\n      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n      <td>583558001</td>\n      <td>0.030255</td>\n      <td>0.084729</td>\n      <td>0.006763</td>\n      <td>0.025407</td>\n      <td>2.601932</td>\n      <td>0.077966</td>\n      <td>86</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1316293</th>\n      <td>2018-09-25</td>\n      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n      <td>666448006</td>\n      <td>0.030255</td>\n      <td>0.084729</td>\n      <td>0.006763</td>\n      <td>0.025407</td>\n      <td>2.601932</td>\n      <td>0.077966</td>\n      <td>86</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18915918</th>\n      <td>2018-09-25</td>\n      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n      <td>521269001</td>\n      <td>0.030255</td>\n      <td>0.084729</td>\n      <td>0.006763</td>\n      <td>0.025407</td>\n      <td>2.601932</td>\n      <td>0.077966</td>\n      <td>86</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2051405</th>\n      <td>2018-09-20</td>\n      <td>ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...</td>\n      <td>918292001</td>\n      <td>0.017532</td>\n      <td>0.042356</td>\n      <td>0.004559</td>\n      <td>0.015237</td>\n      <td>0.788932</td>\n      <td>0.037797</td>\n      <td>45</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2243670</th>\n      <td>2018-09-20</td>\n      <td>ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...</td>\n      <td>751471043</td>\n      <td>0.017532</td>\n      <td>0.042356</td>\n      <td>0.004559</td>\n      <td>0.015237</td>\n      <td>0.788932</td>\n      <td>0.037797</td>\n      <td>45</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3116205</th>\n      <td>2018-09-20</td>\n      <td>ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...</td>\n      <td>794819001</td>\n      <td>0.017532</td>\n      <td>0.042356</td>\n      <td>0.004559</td>\n      <td>0.015237</td>\n      <td>0.788932</td>\n      <td>0.037797</td>\n      <td>45</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3125001</th>\n      <td>2018-09-20</td>\n      <td>ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...</td>\n      <td>884081001</td>\n      <td>0.017532</td>\n      <td>0.042356</td>\n      <td>0.004559</td>\n      <td>0.015237</td>\n      <td>0.788932</td>\n      <td>0.037797</td>\n      <td>45</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3128091</th>\n      <td>2018-09-20</td>\n      <td>ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...</td>\n      <td>689365050</td>\n      <td>0.017532</td>\n      <td>0.042356</td>\n      <td>0.004559</td>\n      <td>0.015237</td>\n      <td>0.788932</td>\n      <td>0.037797</td>\n      <td>45</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4108998 rows  87 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Sum of query counts:\", sum(train_baskets))\nprint(\"Number of data points in the training set:\", len(train))","metadata":{"execution":{"iopub.status.busy":"2024-01-10T07:07:12.716861Z","iopub.execute_input":"2024-01-10T07:07:12.717221Z","iopub.status.idle":"2024-01-10T07:07:12.752583Z","shell.execute_reply.started":"2024-01-10T07:07:12.717191Z","shell.execute_reply":"2024-01-10T07:07:12.751376Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of query counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(\u001b[43mtrain_baskets\u001b[49m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of data points in the training set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train))\n","\u001b[0;31mNameError\u001b[0m: name 'train_baskets' is not defined"],"ename":"NameError","evalue":"name 'train_baskets' is not defined","output_type":"error"}]},{"cell_type":"code","source":"ranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    num_leaves = 20,\n    boosting_type=\"dart\",\n    max_depth=15,\n    n_estimators=500,\n    importance_type='gain',\n    verbose=10\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranker = ranker.fit(\n    train.drop(columns = ['t_dat', 'customer_id', 'article_id', 'label']),\n    train.pop('label'),\n    group=train_baskets,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nsubmission_df = pd.read_csv(os.path.join(BASE_IN_PATH, \"sample_submission.csv\"))\ncandidates = prepare_candidates(submission_df.customer_id.unique(), 12)\ncandidates = (\n    candidates\n    .merge(user_features, on = ('customer_id'))\n    .merge(item_features, on = ('article_id'))\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nbatch_size = 1_000_000\nfor bucket in tqdm(range(0, len(candidates), batch_size)):\n  outputs = ranker.predict(\n      candidates.iloc[bucket: bucket+batch_size]\n      .drop(columns = ['customer_id', 'article_id'])\n      )\n  preds.append(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.concatenate(preds)\ncandidates['preds'] = preds\npreds = candidates[['customer_id', 'article_id', 'preds']]\npreds.sort_values(['customer_id', 'preds'], ascending=False, inplace = True)\npreds = (\n    preds\n    .groupby('customer_id')[['article_id']]\n    .aggregate(lambda x: x.tolist())\n)\npreds['article_id'] = preds['article_id'].apply(lambda x: ' '.join(['0'+str(k) for k in x]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = submission_df[['customer_id']].merge(\n    preds\n    .reset_index()\n    .rename(columns = {'article_id': 'prediction'}), how = 'left')\npreds['prediction'].fillna(' '.join(['0'+str(art) for art in dummy_list_2w]), inplace = True)\npreds.to_csv('submisssion_lightgbm_21_500.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nsub0 = pd.read_csv('/kaggle/input/sub-ensemble/submission (1).csv').sort_values('customer_id').reset_index(drop=True)\nsub1 = pd.read_csv('/kaggle/input/sub-ensemble/submission (2).csv').sort_values('customer_id').reset_index(drop=True)\nsub2 = pd.read_csv('/kaggle/input/sub-ensemble/submission.csv').sort_values('customer_id').reset_index(drop=True)\nsub3 = pd.read_csv('/kaggle/input/sub-ensemble/submissions.csv').sort_values('customer_id').reset_index(drop=True)\nsub4 = pd.read_csv('/kaggle/input/sub-gbm/submisssion_lightgbm_15_400.csv').sort_values('customer_id').reset_index(drop=True)\nsub5 = pd.read_csv('/kaggle/input/sub-gbm/submisssion_lightgbm_5_200.csv').sort_values('customer_id').reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub0.columns = ['customer_id', 'prediction0']\nsub0['prediction1'] = sub1['prediction']\nsub0['prediction2'] = sub2['prediction']\nsub0['prediction3'] = sub3['prediction']\nsub0['prediction4'] = sub4['prediction']\nsub0['prediction5'] = sub5['prediction']\ndel sub1, sub2, sub3, sub4, sub5\nsub0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cust_blend(dt, W = [1,1,1,1,1,1]):\n    #Global ensemble weights\n    #W = [1.15,0.95,0.85]\n    \n    #Create a list of all model predictions\n    REC = []\n    REC.append(dt['prediction0'].split())\n    REC.append(dt['prediction1'].split())\n    REC.append(dt['prediction2'].split())\n    REC.append(dt['prediction3'].split())\n    REC.append(dt['prediction4'].split())\n    REC.append(dt['prediction5'].split())\n    \n    #Create a dictionary of items recommended. \n    #Assign a weight according the order of appearance and multiply by global weights\n    res = {}\n    for M in range(len(REC)):\n        for n, v in enumerate(REC[M]):\n            if v in res:\n                res[v] += (W[M]/(n+1))\n            else:\n                res[v] = (W[M]/(n+1))\n    \n    # Sort dictionary by item weights\n    res = list(dict(sorted(res.items(), key=lambda item: -item[1])).keys())\n    \n    # Return the top 12 itens only\n    return ' '.join(res[:12])\n\nsub0['prediction'] = sub0.apply(cust_blend, W = [1.05,1.00,0.9,1.05,1.00, 1.00], axis=1)\nsub0.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del sub0['prediction0']\ndel sub0['prediction1']\ndel sub0['prediction2']\ndel sub0['prediction3']\ndel sub0['prediction4']\ndel sub0['prediction5']\nsub0.to_csv('submission-blend.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}